{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import trange, tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data.dataloader as loader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from train_dataset import DataServoStereo\n",
    "import train_model as model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GPU or CPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "with open(\"cfg/train_real_images.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "gpu_enabled = config[\"gpu\"]\n",
    "\n",
    "if gpu_enabled:\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"Training on CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = yaml.load(open(\"cfg/train_real_images.yaml\", 'r'), yaml.Loader)\n",
    "arg = namedtuple('Arg', arg.keys())(**arg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_enabled == True:\n",
    "    cudnn.enabled = True\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kper = model.KeyPointGaussian(arg.sigma_kp[0], (arg.num_keypoint, *arg.im_size[1]))\n",
    "if gpu_enabled ==True:\n",
    "    enc = model.Encoder(arg.num_input, arg.num_keypoint, arg.growth_rate[0], arg.blk_cfg_enc, arg.drop_rate, kper).cuda()\n",
    "else:   \n",
    "    enc = model.Encoder(arg.num_input, arg.num_keypoint, arg.growth_rate[0], arg.blk_cfg_enc, arg.drop_rate, kper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam([{'params': enc.parameters(),\n",
    "                           'weight_decay': arg.wd[0]}],\n",
    "                         lr=arg.lr, amsgrad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to adjust the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(ep, ep_train, bn=True):\n",
    "    # Check the value of the argument lr_anne and set the learning rate accordingly\n",
    "    if arg.lr_anne == 'step':\n",
    "        # Use a step function to adjust the learning rate\n",
    "        a_lr = 0.4 ** ((ep > (0.3 * ep_train)) +\n",
    "                       (ep > (0.6 * ep_train)) +\n",
    "                       (ep > (0.9 * ep_train)))\n",
    "    elif arg.lr_anne == 'cosine':\n",
    "        # Use a cosine function to adjust the learning rate\n",
    "        a_lr = (np.cos(np.pi * ep / ep_train) + 1) / 2\n",
    "    elif arg.lr_anne == 'repeat':\n",
    "        # Use a repeated cosine function to adjust the learning rate\n",
    "        partition = [0, 0.15, 0.30, 0.45, 0.6, 0.8, 1.0]\n",
    "        par = int(np.digitize(ep * 1. / ep_train, partition))\n",
    "        T = (partition[par] - partition[par - 1]) * ep_train\n",
    "        t = ep - partition[par - 1] * ep_train\n",
    "        a_lr = 0.5 * (1 + np.cos(np.pi * t / T))\n",
    "        a_lr *= 1 - partition[par - 1]\n",
    "    else:\n",
    "        # Use a constant learning rate\n",
    "        a_lr = 1\n",
    "\n",
    "    # Set the learning rate for all parameter groups in the optimizer\n",
    "    for param_group in optim.param_groups:\n",
    "        param_group['lr'] = max(a_lr, 0.01) * arg.lr\n",
    "\n",
    "    # If bn is True, adjust the momentum of batch normalization layers\n",
    "    if bn:\n",
    "        def fn(m):\n",
    "            if isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d)):\n",
    "                # Set the momentum of batch normalization layers to the current learning rate\n",
    "                m.momentum = min(max(a_lr, 0.01), 0.9)\n",
    "        enc.apply(fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition of the training_function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ep, loader_train):\n",
    "    \n",
    "    # iterate over the training data loader\n",
    "    #for i, (inL0,outS_Tensor,outS) in enumerate(loader_train):\n",
    "    for i, (img,plug_mask_tensor,plug_mask) in enumerate(loader_train):\n",
    "\n",
    "        # enable GPU if enabled in arguments\n",
    "        if gpu_enabled == True:\n",
    "            img = img.cuda()\n",
    "            plug_mask_tensor = plug_mask_tensor.cuda()\n",
    "\n",
    "        # calculate the iteration count and total iterations for the current epoch\n",
    "        ith = ep * len(loader_train.dataset) // arg.batch_size + i, \\\n",
    "            arg.ep_train * len(loader_train.dataset) // arg.batch_size\n",
    "        \n",
    "        # update learning rate based on the scheduler and current iteration count\n",
    "        adjust_lr(*ith)\n",
    "\n",
    "        # update kp sigma\n",
    "        kper.sigma = min(2.0 * ith[0] / ith[1], 1) * (arg.sigma_kp[1] - arg.sigma_kp[0]) + arg.sigma_kp[0]\n",
    "\n",
    "        # generate key points for the input image\n",
    "        keypL0 = enc(img)\n",
    "\n",
    "        # calculate the concentration loss, which concentrates feature points around the edges of the object\n",
    "        # (not on the object itself due to the lack of object detection)\n",
    "        lossC = None\n",
    "        if arg.concentrate != 0:\n",
    "            lossC = []\n",
    "            for idx_i in range(0, arg.num_keypoint - 1):\n",
    "                for idx_j in range(idx_i + 1, arg.num_keypoint):\n",
    "                    distL = torch.norm(torch.cat(\n",
    "                        ((keypL0[0][:, idx_i] - keypL0[0][:, idx_j]).unsqueeze(1),\n",
    "                        (keypL0[0][:, idx_i + arg.num_keypoint] - keypL0[0][:, idx_j + arg.num_keypoint]).unsqueeze(1)),\n",
    "                        dim=1), dim=1)\n",
    "                    lossC.append(distL.mul(arg.concentrate).exp().mul(keypL0[0][:, idx_i + 2 * arg.num_keypoint] *\n",
    "                                                                    keypL0[0][:, idx_j + 2 * arg.num_keypoint]).mean())\n",
    "            lossC = sum(lossC) / len(lossC)\n",
    "        \n",
    "        # calculate the inside loss, which forces the key points to be within the object boundaries\n",
    "        lossI = None\n",
    "        if arg.inside != 0:\n",
    "            inoutL = plug_mask_tensor.eq(0).float()\n",
    "            inoutL = F.interpolate(inoutL.unsqueeze(1), size=keypL0[1].size()[2:], align_corners=False, mode='bilinear')\n",
    "            lossI = arg.inside * (inoutL.mul(keypL0[1]).mean()) \n",
    "\n",
    "        # set the gradients of all optimizer variables to zero\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # calculate and backpropagate the total loss\n",
    "        sum([l for l in [lossC,lossI] if l is not None]).backward()\n",
    "\n",
    "        # update the optimizer variables\n",
    "        optim.step()\n",
    "\n",
    "        # print the loss for the current epoch\n",
    "        if i == 0:\n",
    "            if arg.concentrate == 0:\n",
    "                tqdm.write('ep: {}, loss_I: {:.5f}  '.format(ep,lossI.item()))\n",
    "            elif arg.inside == 0:\n",
    "                tqdm.write('ep: {}, loss_C: {:.5f}  '.format(ep,lossC.item()))\n",
    "            else:\n",
    "                tqdm.write('ep: {}, loss_C loss_I: {:.5f} {:.5f} '.format(ep,lossC.item(), lossI.item()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(base_dir):\n",
    "    state = {'enc_state_dict': enc.state_dict()}\n",
    "    torch.save(state, os.path.join(base_dir, 'ckpt.pth'))\n",
    "    print('checkpoint saved.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main-function for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train():\n",
    "    if arg.task in ['full']:\n",
    "        # create directory to save data\n",
    "        if not os.path.exists(arg.dir_base):\n",
    "            os.makedirs(arg.dir_base)\n",
    "        # copy the configuration file to the created directory\n",
    "        os.system('cp {} {}'.format(\"cfg/train_real_images.yaml\" ,os.path.join(arg.dir_base, 'servo.yaml')))\n",
    "\n",
    "        # check if grayscale or RGB images are used for training and load the corresponding dataset\n",
    "        if arg.num_input == 1:\n",
    "            print(\"Training with grayscale images\")\n",
    "            ds_train = DataServoStereo(arg,grey=True)\n",
    "        else:\n",
    "            print(\"Training with RGB images\")\n",
    "            ds_train = DataServoStereo(arg,grey=False)\n",
    "\n",
    "        # set parameters for the data loader\n",
    "        data_param = {'pin_memory': False, 'shuffle': True, 'batch_size': arg.batch_size, 'drop_last': True,\n",
    "                      'num_workers': 8, 'worker_init_fn': lambda _: np.random.seed(ord(os.urandom(1)))}\n",
    "        \n",
    "        # create data loader for training dataset\n",
    "        loader_train = loader.DataLoader(ds_train, **data_param)\n",
    "\n",
    "        # set the encoder model to training mode\n",
    "        enc.train()\n",
    "        print('training...')\n",
    "        # train for each epoch\n",
    "        for ep in trange(arg.ep_train):\n",
    "            train(ep, loader_train)\n",
    "\n",
    "        # save the trained model checkpoint\n",
    "        save_checkpoint(arg.dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with grayscale images\n",
      "160 training data loaded\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, loss_C loss_I: 0.35768 0.00359 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [09:26<3:33:18, 556.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1, loss_C loss_I: 0.00443 0.00298 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/24 [18:16<3:18:36, 541.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 2, loss_C loss_I: 0.00101 0.00266 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [27:00<3:06:58, 534.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 3, loss_C loss_I: 0.00039 0.00241 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [35:55<2:58:08, 534.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 4, loss_C loss_I: 0.00086 0.00220 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [45:10<2:51:21, 541.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 5, loss_C loss_I: 0.00042 0.00195 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 6/24 [54:02<2:41:30, 538.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 6, loss_C loss_I: 0.00011 0.00184 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7/24 [1:02:45<2:31:07, 533.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 7, loss_C loss_I: 0.00019 0.00166 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [1:11:19<2:20:35, 527.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 8, loss_C loss_I: 0.00005 0.00160 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9/24 [1:19:51<2:10:40, 522.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 9, loss_C loss_I: 0.00004 0.00149 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10/24 [1:28:27<2:01:24, 520.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 10, loss_C loss_I: 0.00010 0.00136 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [1:36:59<1:52:17, 518.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 11, loss_C loss_I: 0.00009 0.00134 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12/24 [1:45:33<1:43:23, 516.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 12, loss_C loss_I: 0.00019 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [1:54:07<1:34:34, 515.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 13, loss_C loss_I: 0.00024 0.00128 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [2:02:41<1:25:52, 515.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 14, loss_C loss_I: 0.00010 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 15/24 [2:11:20<1:17:25, 516.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 15, loss_C loss_I: 0.00048 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 16/24 [2:19:54<1:08:46, 515.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 16, loss_C loss_I: 0.00006 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [2:28:35<1:00:20, 517.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 17, loss_C loss_I: 0.00005 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 18/24 [2:37:08<51:36, 516.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 18, loss_C loss_I: 0.00005 0.00126 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19/24 [2:45:52<43:12, 518.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 19, loss_C loss_I: 0.00008 0.00125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20/24 [2:54:24<34:26, 516.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 20, loss_C loss_I: 0.00006 0.00125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 21/24 [3:02:56<25:45, 515.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 21, loss_C loss_I: 0.00004 0.00125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [3:11:29<17:08, 514.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 22, loss_C loss_I: 0.00003 0.00124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 23/24 [3:20:02<08:33, 513.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 23, loss_C loss_I: 0.00002 0.00124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [3:28:26<00:00, 521.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute \n",
    "main_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
