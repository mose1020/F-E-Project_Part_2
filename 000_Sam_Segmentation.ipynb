{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "#import csv\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "\n",
    "from skimage import measure\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from PIL import Image,ImageOps\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "Attention: \n",
    "* Depending on the height of the camera when recording, the min/max number of pixels must be adjusted.\n",
    "* If another plug is used, the default_ratio must be adjusted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/testfolder_segmentation'\n",
    "path = path + '/'\n",
    "device = \"cuda\"\n",
    "default_ratio = 3.5\n",
    "min_pixel = 12000 # für höhe 100 sehr guter wert: 12000\n",
    "max_pixel = 35000 # für höhe 100 sehr guter wert: 24000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"default\"](checkpoint=\"sam_model/sam_vit_h_4b8939.pth\")\n",
    "sam.to(device=device)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder for Segementation\n",
    "\n",
    "Attention: This functions delets all segmentation images in the path, if the path contains them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(path,'segmentation_images')):\n",
    "    os.mkdir(os.path.join(path,'segmentation_images'))\n",
    "if not os.path.exists(os.path.join(path,'segmentation_error')):\n",
    "    os.mkdir(os.path.join(path,'segmentation_error'))\n",
    "\n",
    "filelist_1 = [ f for f in os.listdir(os.path.join(path,'segmentation_images'))]\n",
    "for f in filelist_1:\n",
    "    os.remove(os.path.join(path,'segmentation_images', f))\n",
    "\n",
    "filelist_2 = [ f for f in os.listdir(os.path.join(path,'segmentation_error'))]\n",
    "for f in filelist_2:\n",
    "    os.remove(os.path.join(path,'segmentation_error', f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions for the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_biggest_contiguous_area(array):\n",
    "    labels,_ = ndimage.label(array)\n",
    "    counts = np.bincount(labels.flatten())\n",
    "    max_label = np.argmax(counts[1:]) + 1\n",
    "    region = (labels == max_label)\n",
    "    \n",
    "    return region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List with all image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = sorted(glob.glob(path+'/train/*')) # normaly glob.glob(path+'/train/*')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sam Segmentation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in tqdm(img_path_list):\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        masks = mask_generator.generate(img)\n",
    "        sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "        ratio_with_mask = [[],[]]\n",
    "\n",
    "        for mask in sorted_masks:\n",
    "            if mask['area'] > min_pixel and mask['area'] < max_pixel:\n",
    "                maybe_plugmask = find_biggest_contiguous_area(mask['segmentation'])\n",
    "                maybe_plugmask = np.array(maybe_plugmask, dtype=np.uint8)\n",
    "                num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(maybe_plugmask)\n",
    "                bbox = stats[1][:4]\n",
    "                x,y,w,h = bbox\n",
    "                ratio_with_mask[0].append(w/h)\n",
    "                ratio_with_mask[1].append(maybe_plugmask)\n",
    "\n",
    "        closest_value = min(ratio_with_mask[0], key=lambda x: abs(x - default_ratio))\n",
    "        closest_index = ratio_with_mask[0].index(closest_value)\n",
    "        plug_mask = ratio_with_mask[1][closest_index]\n",
    "\n",
    "        plug_image = Image.fromarray(plug_mask.astype('uint8') * 255, mode='L')\n",
    "        img_name = img_path.split('/')[-1]\n",
    "        plug_image.save(path+'/segmentation_images/'+img_name)\n",
    "    except:\n",
    "        shutil.move(img_path, path+'/segmentation_error/'+img_name)\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test single images with different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size =[192,192] # [height, width]\n",
    "number = 34360 # number of image to show\n",
    "\n",
    "mask_path = path+'segmentation_images/picture_'+str(number)+'.png'\n",
    "img_path = path+'train/picture_'+str(number)+'.png'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wichtige Info: img_patch und img_segmentation2 sind die gleichen Funktionen wie in train_dataset.py --> wurden hier nur als einzelne Funktionen definiert, da diese in train_dataset.py teil einer Klasse sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_patch(img,segmentation=False):\n",
    "        # Calculate the aspect ratio of the image\n",
    "        img_ratio = img.width / img.height\n",
    "\n",
    "        # If the image is wider than it is high\n",
    "        if img_ratio > 1:\n",
    "            # Scale the width to the desired size and calculate the height while maintaining the aspect ratio\n",
    "            new_width = im_size[0]\n",
    "            new_height = int(new_width / img_ratio)\n",
    "        # If the image is taller than it is wide\n",
    "        else:\n",
    "            # Scale the height to the desired size and calculate the width while maintaining the aspect ratio\n",
    "            new_height = im_size[1]\n",
    "            new_width = int(new_height * img_ratio)\n",
    "\n",
    "        # Resize the image to the calculated size while maintaining the aspect ratio\n",
    "        img = img.resize((new_width, new_height), resample=Image.BICUBIC)\n",
    "\n",
    "        # Add black borders to the left and right or top and bottom of the image to make it the desired size\n",
    "        delta_w = im_size[0] - new_width\n",
    "        delta_h = im_size[1] - new_height\n",
    "        pad_width = delta_w // 2\n",
    "        pad_height = delta_h // 2\n",
    "        padding = (pad_width, pad_height, delta_w - pad_width, delta_h - pad_height)\n",
    "        if segmentation:\n",
    "            img = ImageOps.expand(img, border=padding, fill=(0))\n",
    "        else:\n",
    "            img = ImageOps.expand(img, border=padding, fill=(255,255,255))\n",
    "\n",
    "        # Return the scaled and centered image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_segmention2(mask_path):\n",
    "\n",
    "    mask = Image.open(mask_path).convert('1')\n",
    "    mask = img_patch(mask,segmentation=True) # problem with the color --> we only need one channel\n",
    "    mask = np.array(mask)\n",
    "    mask_tensor = torch.from_numpy(mask)\n",
    "\n",
    "    return mask_tensor, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_path)\n",
    "img = img_patch(img)\n",
    "img = np.array(img)\n",
    "display(Image.fromarray(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,mask = img_segmention2(mask_path)\n",
    "display(Image.fromarray(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.zeros_like(img)\n",
    "result_array[mask] = img[mask]\n",
    "result_image = Image.fromarray(result_array)\n",
    "display(result_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
